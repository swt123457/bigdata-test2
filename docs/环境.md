

## Spark Streaming && Flink

### 流式任务 socket 文本数据调试生成
    linux:   nc -lk 9999
    windows: nc -l -p 9999
    


## Hbase

	
	4. create 'tableName', {NAME => 'cf1', VERSIONS => 1, TTL => 2592000, BLOCKCACHE => true}, {NAME => 'cf2'}
		create 'tableName', {NAME => 'cf1', VERSIONS => 1, TTL => 2592000}, SPLITS => ['20', '40', '60', '80']



### Phoenix
#### 先在 Hbase 建表，以便于设置预分区，及表列族属性，然后在 Phoenix 建映射表（注意表名、列族名和字段名的大小写问题, 加""）
	1. 建表
	create table "db_test"(
	rowkey varchar primary key,
	"cf1"."name" varchar,
	"cf1"."age" varchar,
	"cf1"."sex" varchar,
	"cf1"."job" varchar 
	)
	COLUMN_ENCODED_BYTES = 0;
	
	create table PHOENIX_TEST (
	id varchar PRIMARY KEY,
	NAME varchar,
	AGE varchar,
	SEX varchar,
	JOB varchar 
	)
	COLUMN_ENCODED_BYTES = 0
	split on ('20', '40', '60', '80');
	
	create table PHOENIX_TEST (
	id varchar PRIMARY KEY,
	NAME varchar,
	AGE varchar,
	SEX varchar,
	JOB varchar 
	)
	COLUMN_ENCODED_BYTES = 0,
	salt_buckets = 5;
	
	create index idx_test1 on PHOENIX_TEST (NAME, SEX)
	COLUMN_ENCODED_BYTES = 0,
	salt_buckets = 5;
	
	create index idx_test1 on PHOENIX_TEST (NAME, SEX)
	COLUMN_ENCODED_BYTES = 0
	split on ('20', '40', '60', '80');
	
	create local index idx_waybill_test on WAYBILL_TEST (id, waybill_number) COLUMN_ENCODED_BYTES = 0;
	-- local索引不能设置分区，local索引本身就是存储在原数据hbase表中，利用原hbase表的分区
	
	drop index idx_test1 on PHOENIX_TEST;
	
	列编码问题
	https://blog.csdn.net/u012551524/article/details/83027848?utm_medium=distribute.pc_relevant_bbs_down.none-task-blog-baidujs-1.nonecase&depth_1-utm_source=distribute.pc_relevant_bbs_down.none-task-blog-baidujs-1.nonecase
	
	2. 插入数据 upsert
	upsert into "db_test" (rowkey, "name", "sex", "job") values('00001','trump','F', 'boss');